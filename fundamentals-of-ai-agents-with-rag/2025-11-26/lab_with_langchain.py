# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')

from ibm_watsonx_ai.foundation_models import ModelInference
from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams
from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes
from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM

model_id = 'ibm/granite-3-2-8b-instruct' 

parameters = {
    GenParams.MAX_NEW_TOKENS: 256,  # this controls the maximum number of tokens in the generated output
    GenParams.TEMPERATURE: 0.5, # this randomness or creativity of the model's responses
}

credentials = {
    "url": "https://us-south.ml.cloud.ibm.com"
}

project_id = "skills-network"

# An interference model is a model that makes predictions or decisions based
# on input data.
model = ModelInference(
    model_id=model_id,
    params=parameters,
    credentials=credentials,
    project_id=project_id
)

msg = model.generate("In a few sentences, explain the theory of relativity.")
print(msg['results'][0]['generated_text'])

# Chat models support assigning distinct roles to messages, helping to distinguish
# between user inputs, system instructions, and assistant responses.

# Wrapping the model with LangChain's WatsonxLLM for easier integration
granite_llm = WatsonxLLM(model = model)
print(granite_llm.invoke("Who is man's best friend?"))

# Chat messages have a role associated with them. The three primary roles are:
# 1. System: Provides context or instructions to the model.
# 2. Human: Represents user inputs or questions.
# 3. AI: Represents the model's responses.
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
messages = [
    SystemMessage(content="You are a helpful assistant that translates English to French."),
    HumanMessage(content="Translate the following sentence to French: 'How are you today?'"),
    AIMessage(content="Comment Ã§a va aujourd'hui?"),
    HumanMessage(content="Translate the following sentence to French: 'What is your name?'")
]

# Prompt templates help translate user input and parameters
# into instructions for a language model. They can be used to
# guide a model's response, helping it understand the context
# and generate relevant and coherent language-based output.

# There are several different types of prompt templates.

# String prompt templates
from langchain_core.prompts import PromptTemplate
string_template = PromptTemplate(
    input_variables=["sentence"],
    template="Translate the following sentence to French: '{sentence}'"
)

string_template.invoke({"sentence": "Where is the nearest restaurant?"})

# Chat prompt templates
from langchain_core.prompts import ChatPromptTemplate
chat_template = ChatPromptTemplate.from_messages([
    SystemMessage(content="You are a helpful assistant that translates English to French."),
    HumanMessage(content="Translate the following sentence to French: '{sentence}'")
])
chat_template.invoke({"sentence": "Where is the nearest restaurant?"})

# Messages place holder
from langchain_core.prompts import MessagesPlaceholder
from langchain_core.messages import HumanMessage

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant"),
    MessagesPlaceholder("msgs")
])

input_ = {"msgs": [HumanMessage(content="What is the day after Tuesday?")]}

prompt.invoke(input_)

# Wrap the prompt and chat model and pass them into a chain, which would invoke the message.
from langchain_core.chains import LLMChain
chain = prompt | granite_llm
response = chain.invoke(input = input_)
print(response)